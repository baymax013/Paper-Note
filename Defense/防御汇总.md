# ASGNN：可兼容GNN的ASMP层
论文：[_https://arxiv.org/abs/2210.01002_](https://arxiv.org/abs/2210.01002)

通过“purify”具有潜在噪声（扰动）的图结构，来提高GNN模型的鲁棒性，大致可分为两类：

1. 预处理输入图结构。例如删除“Jaccard相似度”较小的边、用给定图的低秩版本作为近似。之后在用纯化图训练GNN模型。
2. 将输入图的邻接矩阵视为未知，通过优化loss函数来“学习”参数化的图结构（也就是图的邻接矩阵）作为代替。

由于一般的特征聚合函数，如总和、加权平均或最大值操作，会被单个离群节点任意扭曲。所以除了“purify”输入图，还可以直接设计更加鲁棒的特征聚合函数，这里可以结合attention机制。

**ASMP，一种同时进行图结构学习和节点特征学习的消息传递方案**。换句话说，ASMP可以理解为标准的消息传递(即H的更新步骤)，附加了自适应调整图结构的操作(即S的更新步骤)。

![优化函数](https://cdn.nlark.com/yuque/0/2023/png/2381046/1672626433507-8a596aaf-7722-4e73-b910-03c4578d7453.png#averageHue=%23f3f3f3&clientId=uddc8b0e1-224d-4&from=paste&height=110&id=u7d399d0a&originHeight=151&originWidth=1018&originalType=binary&ratio=1&rotation=0&showTitle=true&size=24357&status=done&style=none&taskId=u185f9a88-db15-41ef-a292-375327f8625&title=%E4%BC%98%E5%8C%96%E5%87%BD%E6%95%B0&width=740.3636363636364 "优化函数")
这里主要分为两个部分：feature learning、structure learning。
feature learning就是基于前时间的研究论文，这些文献认为_GNN模型中用于特征学习的消息传递层可以统一解释为最小化某些能量函数的梯度步骤，等价于解决特定的GSD问题_。例如下方APPNP模型中的消息传递，就可以看作为“H(0)=X, η=0.5”的GSD问题，也就是(4)式中的feature learning部分。
![APPNP的消息传递方案](https://cdn.nlark.com/yuque/0/2023/png/2381046/1672626836407-da370536-4b77-42ca-b513-eb11940c5a9b.png#averageHue=%23f7f7f7&clientId=uddc8b0e1-224d-4&from=paste&height=49&id=u8b718727&originHeight=67&originWidth=1005&originalType=binary&ratio=1&rotation=0&showTitle=true&size=11274&status=done&style=none&taskId=ub8397b56-131f-4291-923f-fd44c6c5a27&title=APPNP%E7%9A%84%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E6%96%B9%E6%A1%88&width=730.9090909090909 "APPNP的消息传递方案")
structure learning，主要也就是后面三项，可以说是这篇paper的创新点。这三项分为代表：结构保真项、1范式、F范式。①结构保真项是因为对抗攻击时往往会限制扰动的大小，即扰动图和原图的邻接矩阵A相差不大。②1范式是为了促进稀疏。③F范式是为了惩罚奇异值，也就是限制奇异值。
_注：现实中的真实图往往是稀疏的，邻接矩阵的奇异值一般较小。而攻击后有噪声的邻接矩阵往往更密集，邻接矩阵的奇异值更大，因此用于促进稀疏性和/或抑制奇异值的图结构正则子在图学习的文献中被广泛采用，这里的“1范式和F范式”，就是两个正则子。_

![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1672627676677-d406fe12-3ae8-44d1-b092-c9901ddf96a7.png#averageHue=%23f9f9f9&clientId=uddc8b0e1-224d-4&from=paste&height=56&id=u2cb72687&originHeight=77&originWidth=1005&originalType=binary&ratio=1&rotation=0&showTitle=false&size=10417&status=done&style=none&taskId=u2215c3a4-ddbe-4fb1-877e-832c176c5d8&title=&width=730.9090909090909)
![ASMP的迭代步骤](https://cdn.nlark.com/yuque/0/2023/png/2381046/1672627661313-ec01a8bf-9521-4d67-bebf-2403f8ac4386.png#averageHue=%23f3f3f3&clientId=uddc8b0e1-224d-4&from=paste&height=80&id=u7d693505&originHeight=110&originWidth=1019&originalType=binary&ratio=1&rotation=0&showTitle=true&size=27821&status=done&style=none&taskId=u1d320a74-41a9-4916-a7af-2eaac94cf51&title=ASMP%E7%9A%84%E8%BF%AD%E4%BB%A3%E6%AD%A5%E9%AA%A4&width=741.0909090909091 "ASMP的迭代步骤")
这里H的迭代就是标准的消息传递步骤，也就是迭代节点特征。ASMP在此基础上多了一个S的迭代，S指代可学习的图邻接矩阵，定义了A的分类，是关于图结构的变量。
# GRV：基于KL散度，衡量模型Robust的一项指标
论文：[_https://arxiv.org/abs/2012.02486_](https://arxiv.org/abs/2012.02486)

1. 目前主流的还是特定任务用特定标签训练出一个GNN，可以概况为：为特定应用场景设计的有监督的端到端模型。但“预训练”的这种思想也可以适用于GNNs，基于无监督/自监督的预训练图表示模型，是目前一个较新的研究方向。
2. 传统的衡量GNNs鲁棒性的方法都是基于标签的，通过增加扰动观察GNNs模型跑完数据后的性能参数，以实验为准。但是对于无监督模型的鲁棒性则无法这样，文中提出了一种名为“**图表示漏洞GRV”**的衡量方法，这是一种基于信息理论的度量，用于量化无监督图编码器的鲁棒性。
3. 在无监督图表示学习的情况下，常用的目标是基于**随机行走**或基于**重建**。这些目标强加了一种归纳偏差，即相邻节点或具有相似属性的节点具有相似的表示。然而，在对抗性攻击下，归纳偏差很容易被打破，导致基于随机行走和基于重建的编码器存在漏洞。
4. MI 衡量表示质量，GRV 衡量模型鲁棒性。“MI”指代“MAX _I(S; e(S))_”，_I(X; Y) _表示KL散度，即两者间相互依赖的程度。
5. 导致MI最小的攻击策略称为最差情况攻击（应该是说攻击程度厉害）。
> “模型表示能力与鲁棒性之间的衡量”的文章
> 《Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. 2019. Robustness may be at odds with accuracy. In ICLR.》
> 《Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan. 2019. Theoretically principled trade-off between robustness and accuracy. arXiv preprint arXiv:1901.08573 (2019), 7472–7482.》
> '''基于baseline添加防御策略的防御模型对于0扰动的图，最后的性能指标一般会比baseline差一些'''

6. GRV直观理解，就是如果攻击前后MI保持足够接近，编码器就是健壮的，即**GRV值越低，意味着编码器对对抗性攻击的抵抗力越强**。鲁棒编码器试图在各种对抗性攻击下保持图数据和编码表示之间的相互依赖。

7. 文章的重点式子：
   1. GRV：

![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1674965261282-d0361c14-35f0-4bbf-a54d-df4cc69d1ea5.png#averageHue=%23f5f5f5&clientId=ue356e149-2e6d-4&from=paste&height=50&id=ud8eb010f&originHeight=63&originWidth=402&originalType=binary&ratio=1&rotation=0&showTitle=false&size=7854&status=done&style=none&taskId=u368f25a0-9882-4f20-bb18-fe43343edb6&title=&width=322)

   2. loss函数：

![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1674965285619-aeefa0c9-cc1d-4db5-8c6e-0a4deb6ed176.png#averageHue=%23f2f2f2&clientId=ue356e149-2e6d-4&from=paste&height=38&id=QR4Cd&originHeight=47&originWidth=466&originalType=binary&ratio=1&rotation=0&showTitle=false&size=7773&status=done&style=none&taskId=ued7cc3b8-ad2d-4503-8df6-53d50355be8&title=&width=372.8)

   3. 基于loss后面的max大小对比，可以讲loss函数细化为两个子问题：

![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1674965349454-c5033840-ee64-466f-9243-dec7e624a1df.png#averageHue=%23f3f3f3&clientId=ue356e149-2e6d-4&from=paste&height=69&id=WvcDC&originHeight=86&originWidth=407&originalType=binary&ratio=1&rotation=0&showTitle=false&size=13041&status=done&style=none&taskId=u5d2e9cbf-f8ef-4f18-9005-f225030420c&title=&width=325.6)

   4. 优化算法伪代码：

![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1674965540830-cf791fe8-de35-4e3a-a76d-c72a05921865.png#averageHue=%23f1f1f1&clientId=ue356e149-2e6d-4&from=paste&height=310&id=u966174e0&originHeight=388&originWidth=547&originalType=binary&ratio=1&rotation=0&showTitle=false&size=60806&status=done&style=none&taskId=ub14de1ab-33dc-401d-9407-f54c163af73&title=&width=437.6)

8. 难点及解决办法：
   1. MI 难以计算，解决办法是基于最近工作Deep Graph Infomax，使用噪声对比类型的目标作为 MI 的近似值，

![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1674965675410-4bfa7376-dbd9-4c63-ad03-e16c1b6215c5.png#averageHue=%23f1f1f1&clientId=ue356e149-2e6d-4&from=paste&height=38&id=ube5d262c&originHeight=47&originWidth=465&originalType=binary&ratio=1&rotation=0&showTitle=false&size=7924&status=done&style=none&taskId=u703076f1-874a-40ab-b5d5-1ac2c5ab988&title=&width=372)

   2. 难以找到最坏情况的_μs* _。解决办法有点没看懂，其中一个是离散的A无法PGD下降，用了图PGD攻击解决。还有一个真实分布用经验分布替换，就是iid的样本。

9. AG，一种传统的鲁棒性，即对抗间隙(AG)，来衡量下游节点分类器的鲁棒性。AG越小，鲁棒越好。注意，AG是基于label空间的。
10. 无监督图表示学习中，最近被提倡的另一种方法是采用**MI最大化原则**。这种类型的方法已经在图上的无监督表示学习的标准指标上取得了巨大的收益，甚至可以与监督学习方案竞争。然而，这些基于MI的图嵌入通常不能很好地处理有噪声或对抗性数据。这促使人们进一步考虑基于MI的图形鲁棒表示学习。
# GSR：基于“Pretrain-Finetune”的图结构优化框架
论文：[_https://arxiv.org/abs/2211.06545_](https://arxiv.org/abs/2211.06545)

1. 这篇论文提出了一个**基于“Pretrain-Finetune”模式的图结构优化框架Graph Structure Refinement，GSR**。目前绝大部分的图学习模型（GNN）都是关于Node表示的学习，而这篇提出的模型是针对Graph Structure，也就是对邻接矩阵A进行的学习，输出是与Node间的Edge有关。

---

2. 预训练部分，由于是对图结构进行学习的模型，所以预训练阶段采用的任务是**链路预测**，而非节点分类。与此对应的，预训练模型最终得到的是关于图结构的学习，模型输出的是一对节点 v_i 和 v_j 之间**Edge的概率**。此外，预训练采用了**自监督SSL、多视图、对比学习**等技术。
> 自我理解：
> 这里预训练模型输出Edge的概率并非是“原图中节点对是否存在Edge的概率”，而是模型所学习到的有关最优图结构的判断。
> 换句话说，由于现实中的图通常十分稀疏，且具有噪声，对于任务获得正确结果而言往往不是最优的图，即**现实图≠最优图**。预训练的图结构学习模型的**最终目标是通过学习，获得判定更优图结构的能力**，所输出的Edge的概率就是所谓的节点间潜在边的概率。即模型认定这一对节点间若存在Edge，会使图结构更优，进而更容易得到任务的最优解或正确解。


---

3. 预训练阶段通过**视图间、视图内的对比学习**来制定总损失：_LP_ = α_Lintra_ + (1-α)_Linter_  ，其中视图间、视图内的损失如下：

![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1678432296399-bfd408b8-6246-4d0e-ad22-8ad9b9780323.png#averageHue=%23f1f1f1&clientId=u7f43ca11-15fe-4&from=paste&height=72&id=u7177f37a&originHeight=91&originWidth=366&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=12527&status=done&style=none&taskId=u0bce90bb-ca2a-4039-8fbe-189faba9711&title=&width=290)
![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1678432306850-9c5fcd98-8326-4450-91dd-288730488c19.png#averageHue=%23f3f3f3&clientId=u7f43ca11-15fe-4&from=paste&height=80&id=u3850e4a3&originHeight=101&originWidth=504&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=15425&status=done&style=none&taskId=u4295aae6-4668-4755-8ac2-c4d30acce32&title=&width=401)

---

4. Finetune阶段，首先比传统的Finetune多做了一件事，就是**先基于预训练的模型优化Downstream Task的图，称为图结构精化**。具体做法是将Downstream Task的图传入预训练的模型中，得到的是节点对间Edge的概率，然后添加top m个最高概率的non-edges（当前不存在的边），删除top m个最小概率的existing-edges（当前存在的边），来得到一个优化后的图。

---

5. Finetune的初始化模型，是“query GNN encoder of node feature view”，即预训练阶段中的节点特征视图Query GNN编码器。

---

6. Finetune阶段的添加/删除边，对于引文网络（Cora、Citeseer）来说，添加边更有用，删除边不太行；而对于社交网络（Reddit、BlogCatalog），删除边比添加边更有用。
# Pro-GNN：联合框架（图结构和图表征）提升鲁棒
“Graph Purifier”图净化思想去做防御，从这个角度来看，关键的挑战是应该遵循什么标准来清理扰动图。
现实中的图有两个特点：**稀疏和低秩**。文章基于metattack扰动后的可视化，观察到：

   1. metattack能增大邻接矩阵的奇异值
   2. metattack快速增加邻接矩阵的秩
   3. 当分别从摄动图中去除敌对边和法向边时，我们观察到去除敌对边比去除法向边降低秩更快
   4. metattack倾向于连接具有较大特征差异的节点

故这篇文章以图的**稀疏性、低秩性和特征平滑性的性质为目标**，设计了鲁棒图神经网络。Pro-GNN是一个联合框架，也就是即学习一个干净图结构S，也同时更新迭代GNN模型参数。

---

对于图结构学习，本质上就是基于图的稀疏性、低秩性为目标学习一个净化的图邻接矩阵S：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1683273019607-e6aa60ad-9c9d-4b6b-9bbd-66f5a01312d4.png#averageHue=%23f8f7f5&clientId=u6034ab1c-5b42-4&from=paste&height=61&id=u43ffbdd5&originHeight=89&originWidth=789&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=15582&status=done&style=none&taskId=u09b70d82-6e6a-4b24-8e86-8a9e32c9817&title=&width=538.2000122070312)
第一项是确保净化后的 S 要尽可能接近原始的 A。后面两项分别对应着最小化“1范式”和“核范式”，因为**最小化矩阵的“1范式”和“核范式”可以分别使矩阵稀疏和低秩。**此外，最小化核范式的另一个重要好处是可以减少每一个奇异值，从而减轻对抗性攻击中奇异值增大的影响。这里后面的s.t.约束表示学习的图是个无向图，因此是对称的。
在此基础上，第二个目的是要保证学习图 S 的特征平滑。特征平滑度可以用Ls来表示。这里用归一化的图拉普拉斯矩阵 L_hat 代替 L：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1683274281280-f45e9114-547f-4e4c-a600-e6482abe1551.png#averageHue=%23f8f7f6&clientId=u6034ab1c-5b42-4&from=paste&height=82&id=u6957557d&originHeight=114&originWidth=574&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=15949&status=done&style=none&taskId=u18cc1932-6c9b-4140-a829-6f52d9ee490&title=&width=414.20001220703125)
其中特征的平方差越大，也就是特征差异越大，那么Ls就会很大，所以Ls越小，图上的特征X就更平滑。整合至 L0 后为：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1683274350867-8b2379ad-5089-4266-85cf-c2243d53746e.png#averageHue=%23f7f6f4&clientId=u6034ab1c-5b42-4&from=paste&height=56&id=u163b899c&originHeight=70&originWidth=669&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=12798&status=done&style=none&taskId=u53cdd6e1-db94-4b4d-a270-0e640b4d32f&title=&width=535.2)

---

至此，一种自然的思路是通过这个loss学习一个净化图结构 S，然后用这个 S 去训练一个GNN，也就是二阶段流程。但是文章指出通过**联合 GSL 和 GNN** 两部分可以有更好的效果，因为这种二阶段式的 S 可能不是下游任务中最优的图结构。联合后的loss为：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1683275480293-57d929d2-0b57-489e-b619-4a0e1ba368ed.png#averageHue=%23f9f8f7&clientId=u6034ab1c-5b42-4&from=paste&height=135&id=u67f85126&originHeight=169&originWidth=764&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=29198&status=done&style=none&taskId=uec81f159-cb19-4929-b8fe-803724c8e66&title=&width=611.2)
在优化 loss 的时候，采用**交替优化**。意思就是更新 θ 时，固定 S 并去除于 θ 无关的项。更新 S 也是一样。此时，更新 θ 就相当于是标准的训练GNN的loss，用SGD或Adam这种就行。更新 S 文章采用的是一种叫做 Forward-Backward splitting methods，这个想法是将梯度下降步骤和近端步骤交替进行。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/2381046/1683277322172-f3533ca1-1342-43fd-a992-1a28cd1f95a4.png#averageHue=%23f5f4f3&clientId=u6034ab1c-5b42-4&from=paste&height=497&id=u962b0d0b&originHeight=621&originWidth=798&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=114837&status=done&style=none&taskId=ueb86fc15-2f5e-4a61-91e1-8d5f8841763&title=&width=638.4)
以上就是Pro-GNN的整个优化迭代算法总结。在第1行中，首先将估计的图S初始化为中毒图A。在第2行中，随机初始化GNN参数。从第3行到第10行，交替迭代地更新S和GNN参数θ。具体来说，在每次迭代中训练GNN参数，同时在每次τ迭代中训练GSL模型。
